# resource-rational-prediction
This is code to recreate Figures 2 and 3 of "Humans are resource-rational predictors in a sequence learning task".

Figure 2, as described by the paper itself: Given a certain amount of cognitive resources, are humans predicting as well as possible? We quantify cognitive resources using a measure of memory previously used for salamander retinal ganglion cells and cultured neurons-- the mutual information between the state of the sensor, proxied by the human's prediction, and the forward-time causal state of the input sequence. The forward-time causal state is the minimal sufficient statistic for prediction of the input sequence, and so the nonlinear correlation between the human's behavior and this minimal sufficient statistic of prediction is an information-theoretic rate that can quantify memory. We quantify predictive power simply by the predictive accuracy. For any given rate, there is a maximal predictive accuracy attainable. We calculate this maximal predictive accuracy and show human behavior relative to the resultant predictive rate-accuracy (PRA) curve. The input sequences are specially designed so that this calculation can be performed. Plotted next to the PRA curve are the actual rates and accuracies of humans; if the humans are close to the curve, then they are resource-rational or efficient predictors. Because we collected a finite set of data from each participant, points can fall above the curve due to chance fluctuations in accuracy scores.

PRA_CogSci.py will produce Figure 2 in the paper, using NoisyPeriodic_PRA.npz, Clumpy_PRA.npz, and EvenProcess_PRA.npz, which you need to download. The beginning part of PRA_CogSci.py shows how the data in those npz files is made, while the later part shows how the figures are made from the data.

Figure 3, as described by the paper itself: A maximum likelihood method ("Inferring an observer's prediction strategy in sequence learning experiments", https://www.mdpi.com/1099-4300/22/8/896) adjusted to become a maximum a posteriori method was used to infer which prediction strategy was being employed by participants. There were four candidates: Bayesian order-$R$ Markov modeling or n-gram observers, Bayesian Structural Inference, Long Short-Term Memory Units, or logistic regression. For all classes of algorithm considered, we assume the participant trains the model on the history of symbols emitted across all previous trials and then uses that model to predict the next trial. We report the "best-fit" model as the one (from this set of four) that minimizes the Akaike Information Criterion (AIC), which is $2K-2\log L$, where $K$ is the number of parameters and $L$ is the probability of the data given the model.

maximum_likelihood.py will produce the data for Figure 3 in the paper, using the algorithm mentioned above. The data is given in the "[process]Data[num].txt" files, where each number corresopnds to a number of times that the LSTM was run for that process. (Each LSTM run produced a slightly different answer due to stochasticity in initial conditions, and the best score was taken.) bargraph.py then compiles the results.
